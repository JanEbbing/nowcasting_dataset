{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf9b8624-4cc8-42fa-aaee-b17df33e7438",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import cfgrib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import datetime\n",
    "import xarray as xr\n",
    "from typing import Union"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04452225-cb01-4988-b870-9907f96447fc",
   "metadata": {},
   "source": [
    "This notebook is research for [GitHub issue #344: Convert NWP grib files to Zarr intermediate](https://github.com/openclimatefix/nowcasting_dataset/issues/344).\n",
    "\n",
    "Useful links:\n",
    "\n",
    "* [Met Office's data docs](http://cedadocs.ceda.ac.uk/1334/1/uk_model_data_sheet_lores1.pdf)\n",
    "\n",
    "Done:\n",
    "\n",
    "* Merge Wholesale1 and 2 (2 includes dswrf, lcc, mcc, and hcc)\n",
    "* Remove dimensions we don't care about (e.g. select temperature at 1 meter, not 0 meters)\n",
    "* Reshape images from 1D to 2D.\n",
    "* Do we really need to convert datetimes to unix epochs before appending to zarr?  If no, submit a bug report.\n",
    "* eccodes takes ages to load multiple datasets from each grib.  Maybe it'd be faster to pre-load each grib into ramdisk?  UPDATE: Nope, it's not faster!  What does give a big speedup, though, is using an idx file!\n",
    "\n",
    "Some outstanding questions / Todo items\n",
    "\n",
    "* Remove `wholesale_file_number`.  And then use a `pd.Series` instead of a DF.\n",
    "* Zarr chunk size and compression.\n",
    "* Do we need to combine all the DataArrays into a single DataArray (with \"variable\" being a dimension?).  The upside is that then a single Zarr chunk can include multiple variables.  The downside is that we lose the metadata (but that's not a huge problem, maybe?)\n",
    "* Reshaping is pretty slow.  Maybe go back to using `np.reshape`?\n",
    "* Separately log \"bad files\" to a CSV file?\n",
    "* Restart from last `time` in existing Zarr.\n",
    "* Convert to float16?\n",
    "* Check for NaNs.  cdcb has NaNs.\n",
    "* Experiment with when it might be fastest to load data into memory.\n",
    "* Parallelise\n",
    "* Convert to script\n",
    "* Use click to set source and target directories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89fc49d7-2806-4277-9019-c33d4e6a3963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define geographical domain for UKV.\n",
    "# Taken from page 4 of http://cedadocs.ceda.ac.uk/1334/1/uk_model_data_sheet_lores1.pdf\n",
    "# To quote the PDF:\n",
    "# \"The United Kingdom domain is a 1,096km x 1,408km ~2km resolution grid.\n",
    "# The OS National Grid corners of the domain are:\"\n",
    "\n",
    "DY_METERS = DX_METERS = 2_000\n",
    "NORTH = 1223_000\n",
    "SOUTH = -185_000\n",
    "WEST = -239_000\n",
    "EAST = 857_000\n",
    "\n",
    "# Note that the UKV NWPs y is top-to-bottom\n",
    "NORTHING = np.arange(start=NORTH, stop=SOUTH, step=-DY_METERS, dtype=np.int32)\n",
    "EASTING = np.arange(start=WEST, stop=EAST, step=DX_METERS, dtype=np.int32)\n",
    "\n",
    "NUM_ROWS = len(NORTHING)\n",
    "NUM_COLS = len(EASTING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c703f9a0-12a9-4a40-b64e-8958efdd267f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NWP_PATH = Path(\"/home/jack/Data/NWP/01\")\n",
    "NWP_PATH = Path(\"/media/jack/128GB_USB/NWPs\")  # Must not include trailing slash!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "26672e96-73da-43a9-af05-bebb160ad46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert NWP_PATH.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c8d33a5-5300-4393-b9f7-4842e0ea0364",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = list(NWP_PATH.glob(\"*/*/*/*Wholesale[12].grib\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4ee7b391-4e8b-4584-af3c-f42d68cd69e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2b4c0761-d45e-496b-8c11-da446daa410f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grib_filename_to_datetime_and_wholesale_file_number(full_filename: Path) -> dict[str, object]:\n",
    "    \"\"\"Parse the grib filename and return the datetime and wholesale number.\n",
    "    \n",
    "    Returns a dict with three keys: 'full_filename', `nwp_init_datetime_utc` and `wholesale_file_number`.\n",
    "      For example, if the filename is '202101010000_u1096_ng_umqv_Wholesale1.grib',\n",
    "      then `nwp_init_datetime_utc` will be datetime(year=2021, month=1, day=1, hour=0, minute=0)\n",
    "      and the `wholesale_file_number` will be 1 (represented as an int).\n",
    "      \n",
    "    Raises RuntimeError if the filename does not match the expected regex pattern.\n",
    "    \"\"\"\n",
    "    # Get the base_filename, which will be of the form '202101010000_u1096_ng_umqv_Wholesale1.grib'\n",
    "    base_filename = full_filename.name\n",
    "\n",
    "    # Use regex to match the year, month, day, hour, minute and wholesale_number.  i.e., group the filename like this:\n",
    "    # '(2021)(01)(01)(00)(00)_u1096_ng_umqv_Wholesale(1)'.\n",
    "    # A quick guide to the relevant regex operators:\n",
    "    #   ^ matches the beginning of the string.\n",
    "    #   () defines a group.\n",
    "    #   (?P<name>...) names the group.  We can access the group with `regex_match.groupdict()[name]`.\n",
    "    #   \\d matches a single digit.\n",
    "    #   {n} matches the preceding item n times.\n",
    "    #   . matches any character.\n",
    "    #   $ matches the end of the string.\n",
    "    regex_pattern_string = (\n",
    "        '^'  # Match the beginning of the string.\n",
    "        '(?P<year>\\d{4})'  # Match the year.\n",
    "        '(?P<month>\\d{2})'  # Match the month.\n",
    "        '(?P<day>\\d{2})'  # Match the day.\n",
    "        '(?P<hour>\\d{2})'  # Match the hour.\n",
    "        '(?P<minute>\\d{2})'  # Match the minute.\n",
    "        '_u1096_ng_umqv_Wholesale'\n",
    "        '(?P<wholesale_file_number>\\d)'  # Match the number after \"Wholesale\".\n",
    "        '\\.grib$'  # Match the end of the string (escape the fullstop).\n",
    "    )\n",
    "    regex_pattern = re.compile(regex_pattern_string)\n",
    "    regex_match = regex_pattern.match(base_filename)\n",
    "    if regex_match is None:\n",
    "        raise RuntimeError(f\"Filename '{full_filename}' does not conform to expected regex pattern '{regex_pattern_string}'!\")\n",
    "\n",
    "    # Convert strings to ints:\n",
    "    regex_groups = {key: int(value) for key, value in regex_match.groupdict().items()}\n",
    "\n",
    "    wholesale_file_number = regex_groups.pop('wholesale_file_number')\n",
    "    dt = datetime.datetime(**regex_groups)\n",
    "    return {\n",
    "        'full_filename': full_filename,\n",
    "        'nwp_init_datetime_utc': dt,\n",
    "        'wholesale_file_number': wholesale_file_number}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "775e59b0-4d52-4700-8ca6-e92c5e7db02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_and_group_grib_filenames(filenames: list[Path]) -> pd.DataFrame:\n",
    "    \"\"\"Returns a DataFrame where the index is the datetime of the NWP init time.\n",
    "    \n",
    "    And the columns of the DataFrame are 'wholesale_file_number' and 'full_filename'.\n",
    "    \"\"\"\n",
    "    n_filenames = len(filenames)\n",
    "    df = pd.DataFrame(\n",
    "        index=range(n_filenames), \n",
    "        columns=['full_filename', 'nwp_init_datetime_utc', 'wholesale_file_number'])\n",
    "    for i, filename in enumerate(filenames):\n",
    "        df.iloc[i] = grib_filename_to_datetime_and_wholesale_file_number(filename)\n",
    "    \n",
    "    # Change dtypes\n",
    "    df = df.astype({'wholesale_file_number': np.int8})\n",
    "    df['nwp_init_datetime_utc'] = pd.to_datetime(df['nwp_init_datetime_utc'])\n",
    "    \n",
    "    # Set index and sort\n",
    "    df = df.set_index('nwp_init_datetime_utc')\n",
    "    df = df.sort_index()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "34867f22-9980-44f8-8e46-25ae79f2eba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_grib_file(full_filename: Union[Path, str], verbose: bool=False) -> xr.Dataset:\n",
    "    \"\"\"Merges and loads all contiguous xr.Datasets from the grib file.\n",
    "    \n",
    "    Removes unnecessary variables.  Picks heightAboveGround=1meter for temperature.\n",
    "    \n",
    "    Returns an xr.Dataset which has been loaded from disk.  Loading from disk at this point\n",
    "    takes about 2 seconds for a 250 MB grib file, but speeds up reshape_1d_to_2d\n",
    "    from about 7 seconds to 0.5 seconds :)\n",
    "    \n",
    "    Args:\n",
    "      full_filename:  The full filename (including the path) of a single grib file.\n",
    "      verbose:  If True then print out some useful debugging information.\n",
    "    \"\"\"\n",
    "    # The grib files are \"heterogeneous\", so we use cfgrib.open_datasets\n",
    "    # to return a list of contiguous xr.Datasets.\n",
    "    # See https://github.com/ecmwf/cfgrib#automatic-filtering\n",
    "    datasets_from_grib = cfgrib.open_datasets(full_filename)\n",
    "    n_datasets = len(datasets_from_grib)\n",
    "\n",
    "    # Get each dataset into the right shape for merging:\n",
    "    # Loop round the datasets using an index (instead of `for ds in datasets_from_grib`)\n",
    "    # because we will be modifying each dataset:\n",
    "    for i in range(n_datasets):\n",
    "        ds = datasets_from_grib[i]\n",
    "\n",
    "        if verbose:\n",
    "            print(\"\\nDataset\", i, \"before processing:\\n\", ds, \"\\n\")\n",
    "        \n",
    "        # For temperature, we want the temperature at 1 meter above ground,\n",
    "        # not at 0 meters above ground.  The early NWPs (definitely in the 2016-03-22 NWPs),\n",
    "        # heightAboveGround only has 1 entry (\"1\" meter above ground) and `heightAboveGround` isn't set as a dimension for `t`.\n",
    "        # In later NWPs, 'heightAboveGround' has 2 values (0, 1) is a dimension for `t`.\n",
    "        if 't' in ds and 'heightAboveGround' in ds['t'].dims:\n",
    "            ds = ds.sel(heightAboveGround=1)\n",
    "\n",
    "        # Delete unnecessary variables.\n",
    "        vars_to_delete = [\n",
    "            'unknown', 'valid_time', 'heightAboveGround', 'heightAboveGroundLayer',\n",
    "            'atmosphere', 'cloudBase', 'surface', 'meanSea', 'level']\n",
    "        for var_name in vars_to_delete:\n",
    "            try:\n",
    "                del ds[var_name]\n",
    "            except KeyError as e:\n",
    "                if verbose:\n",
    "                    print('var name not in dataset:', e)\n",
    "            else:\n",
    "                if verbose:\n",
    "                    print('Deleted', var_name)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"\\nDataset\", i, \"after processing:\\n\", ds, \"\\n\")\n",
    "            print(\"**************************************************\")\n",
    "            \n",
    "        datasets_from_grib[i] = ds\n",
    "        del ds\n",
    "    \n",
    "    merged_ds = xr.merge(datasets_from_grib)\n",
    "    del datasets_from_grib  # Save memory\n",
    "    return merged_ds.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bd66015d-b4db-4e01-a82e-72726bbc6a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_1d_to_2d(dataset: xr.Dataset) -> xr.Dataset:\n",
    "    \"\"\"Convert 1D into 2D array.\n",
    "    \n",
    "    For each `step`, the pixel values in the grib files represent a 2D image.  But, in the grib,\n",
    "    the values are in a flat 1D array (indexed by the `values` dimension).  The ordering of the pixels are row_0, row_1, row_2, etc.\n",
    "\n",
    "    We reshape every data variable at once using this trick.\n",
    "    \"\"\"\n",
    "    # Adapted from https://stackoverflow.com/a/62667154\n",
    "    \n",
    "    # Don't reshape yet.  Instead just create new coordinates,\n",
    "    # which give the `x` and `y` position of each position in the `values` dimension:\n",
    "    dataset = dataset.assign_coords(\n",
    "        {\n",
    "            'x': ('values', np.tile(EASTING, reps=NUM_ROWS)), \n",
    "            'y': ('values', np.repeat(NORTHING, repeats=NUM_COLS))\n",
    "        })\n",
    "    \n",
    "    # Now set \"values\" to be a MultiIndex, indexed by `y` and `x`:\n",
    "    dataset = dataset.set_index(values=(\"y\", \"x\"))\n",
    "    \n",
    "    # Now unstack.  This gets rid of the `values` dimension and indexes\n",
    "    # the data variables using `y` and `x`.\n",
    "    return dataset.unstack(\"values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e42183be-67de-4ced-8a72-4ec61b3b8999",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_has_variables(dataset: xr.Dataset) -> bool:\n",
    "    \"\"\"Return True if `dataset` has at least one variable.\"\"\"\n",
    "    return len(dataset.variables) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "52b89ce1-7d2d-4bc0-b7f5-462ae40eb4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_to_zarr(dataset: xr.Dataset, zarr_path: Union[str, Path]):\n",
    "    print(\"Writing to disk...\")\n",
    "    zarr_path = Path(zarr_path)\n",
    "    if zarr_path.exists():\n",
    "        to_zarr_kwargs = dict(\n",
    "            append_dim = \"time\",\n",
    "        )\n",
    "    else:\n",
    "        to_zarr_kwargs = dict(\n",
    "            # Need to manually set the time units otherwise xarray defaults to using\n",
    "            # units of *days* (and hence cannot represent sub-day temporal resolution), which corrupts\n",
    "            # the `time` values when we appending to Zarr.  See:\n",
    "            # https://github.com/pydata/xarray/issues/5969 and\n",
    "            # http://xarray.pydata.org/en/stable/user-guide/io.html#time-units\n",
    "            encoding={\n",
    "                'time': {\n",
    "                    'units': 'nanoseconds since 1970-01-01'\n",
    "                },\n",
    "            },\n",
    "        )\n",
    "\n",
    "    dataset.to_zarr(zarr_path, **to_zarr_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6aff43d0-3e8b-4491-b781-a53f2da113eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_grib_for_single_nwp_init_time(full_filenames: list[Path]) -> xr.Dataset:\n",
    "    datasets_for_nwp_init_datetime = []\n",
    "    for full_filename in full_filenames:\n",
    "        print('Loading', full_filename)\n",
    "        try:\n",
    "            dataset_for_filename = load_grib_file(full_filename)\n",
    "        except EOFError as e:\n",
    "            print(e, f\"Filesize = {full_filename.stat().st_size:,d} bytes\")\n",
    "            # If any of the files associated with this nwp_init_datetime is broken then\n",
    "            # skip all, because we don't want incomplete data for an init_datetime.\n",
    "            datasets_for_nwp_init_datetime = []\n",
    "            break\n",
    "        else:\n",
    "            if dataset_has_variables(dataset_for_filename):\n",
    "                datasets_for_nwp_init_datetime.append(dataset_for_filename)\n",
    "    if len(datasets_for_nwp_init_datetime) == 0:\n",
    "        return\n",
    "    print(\"Merging...\")\n",
    "    dataset_for_nwp_init_datetime = xr.merge(datasets_for_nwp_init_datetime)\n",
    "    del datasets_for_nwp_init_datetime\n",
    "    print(\"Reshaping...\")\n",
    "    dataset_for_nwp_init_datetime = reshape_1d_to_2d(dataset_for_nwp_init_datetime)\n",
    "    return dataset_for_nwp_init_datetime.expand_dims(\"time\", axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a39717ed-e5ed-4b79-8375-47f0315fb24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = decode_and_group_grib_filenames(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "a9e44690-83fb-4584-b84c-94c164abe55f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ecCodes provides no latitudes/longitudes for gridType='transverse_mercator'\n",
      "ecCodes provides no latitudes/longitudes for gridType='transverse_mercator'\n",
      "ecCodes provides no latitudes/longitudes for gridType='transverse_mercator'\n",
      "ecCodes provides no latitudes/longitudes for gridType='transverse_mercator'\n",
      "ecCodes provides no latitudes/longitudes for gridType='transverse_mercator'\n",
      "ecCodes provides no latitudes/longitudes for gridType='transverse_mercator'\n",
      "ecCodes provides no latitudes/longitudes for gridType='transverse_mercator'\n",
      "ecCodes provides no latitudes/longitudes for gridType='transverse_mercator'\n",
      "ecCodes provides no latitudes/longitudes for gridType='transverse_mercator'\n",
      "ecCodes provides no latitudes/longitudes for gridType='transverse_mercator'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.95 s, sys: 272 ms, total: 2.22 s\n",
      "Wall time: 2.24 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "full_filename = Path(\"/media/jack/128GB_USB/NWPs/2016/03/22/201603222100_u1096_ng_umqv_Wholesale1.grib\")\n",
    "ds = load_grib_file(full_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "423307b8-681a-42fd-bd41-cbf7fee9b00d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 369 ms, sys: 51.4 ms, total: 420 ms\n",
      "Wall time: 419 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ds_reshaped = reshape_1d_to_2d(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "362eb79c-3912-4517-9e43-7124270d03a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /media/jack/128GB_USB/NWPs/2016/03/22/201603221500_u1096_ng_umqv_Wholesale1.grib\n",
      "No valid message found in file: '/media/jack/128GB_USB/NWPs/2016/03/22/201603221500_u1096_ng_umqv_Wholesale1.grib' Filesize = 0 bytes\n",
      "Loading /media/jack/128GB_USB/NWPs/2016/03/22/201603221800_u1096_ng_umqv_Wholesale1.grib\n",
      "No valid message found in file: '/media/jack/128GB_USB/NWPs/2016/03/22/201603221800_u1096_ng_umqv_Wholesale1.grib' Filesize = 0 bytes\n",
      "Loading /media/jack/128GB_USB/NWPs/2016/03/22/201603222100_u1096_ng_umqv_Wholesale2.grib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ecCodes provides no latitudes/longitudes for gridType='transverse_mercator'\n",
      "ecCodes provides no latitudes/longitudes for gridType='transverse_mercator'\n",
      "ecCodes provides no latitudes/longitudes for gridType='transverse_mercator'\n",
      "ecCodes provides no latitudes/longitudes for gridType='transverse_mercator'\n",
      "ecCodes provides no latitudes/longitudes for gridType='transverse_mercator'\n",
      "ecCodes provides no latitudes/longitudes for gridType='transverse_mercator'\n",
      "ecCodes provides no latitudes/longitudes for gridType='transverse_mercator'\n",
      "ecCodes provides no latitudes/longitudes for gridType='transverse_mercator'\n",
      "ecCodes provides no latitudes/longitudes for gridType='transverse_mercator'\n",
      "ecCodes provides no latitudes/longitudes for gridType='transverse_mercator'\n",
      "ecCodes provides no latitudes/longitudes for gridType='transverse_mercator'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /media/jack/128GB_USB/NWPs/2016/03/22/201603222100_u1096_ng_umqv_Wholesale1.grib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ecCodes provides no latitudes/longitudes for gridType='transverse_mercator'\n",
      "ecCodes provides no latitudes/longitudes for gridType='transverse_mercator'\n",
      "ecCodes provides no latitudes/longitudes for gridType='transverse_mercator'\n",
      "ecCodes provides no latitudes/longitudes for gridType='transverse_mercator'\n",
      "ecCodes provides no latitudes/longitudes for gridType='transverse_mercator'\n",
      "ecCodes provides no latitudes/longitudes for gridType='transverse_mercator'\n",
      "ecCodes provides no latitudes/longitudes for gridType='transverse_mercator'\n",
      "ecCodes provides no latitudes/longitudes for gridType='transverse_mercator'\n",
      "ecCodes provides no latitudes/longitudes for gridType='transverse_mercator'\n",
      "ecCodes provides no latitudes/longitudes for gridType='transverse_mercator'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging...\n",
      "Reshaping...\n"
     ]
    }
   ],
   "source": [
    "for nwp_init_datetime_utc in df.index.unique():\n",
    "    full_filenames = df.loc[nwp_init_datetime_utc]\n",
    "    if full_filenames.shape != (2, 2):  # TODO: Change this back to len(full_filenames) after getting rid of `wholesale_file_number`.\n",
    "        print(len(full_filenames), \"filenames found for\", nwp_init_datetime_utc, \".  Expected 2. Skipping!\")\n",
    "        continue\n",
    "    full_filenames = full_filenames.full_filename.values\n",
    "    ds = load_grib_for_single_nwp_init_time(full_filenames)\n",
    "    if ds is not None:\n",
    "        break # TODO: Remove!\n",
    "        append_to_zarr(ds, \"/home/jack/data/nwp.zarr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b366603a-3421-4621-a6bb-33910956d893",
   "metadata": {},
   "source": [
    "## Load from Zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "633fb9bd-da4b-4e06-acec-bc09bd41b214",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15068/856980436.py:1: RuntimeWarning: Failed to open Zarr store with consolidated metadata, falling back to try reading non-consolidated metadata. This is typically much slower for opening a dataset. To silence this warning, consider:\n",
      "1. Consolidating metadata in this existing store with zarr.consolidate_metadata().\n",
      "2. Explicitly setting consolidated=False, to avoid trying to read consolidate metadata, or\n",
      "3. Explicitly setting consolidated=True, to raise an error in this case instead of falling back to try reading non-consolidated metadata.\n",
      "  ds_from_zarr = xr.open_dataset(\"/home/jack/data/nwp.zarr\", engine=\"zarr\")\n"
     ]
    },
    {
     "ename": "GroupNotFoundError",
     "evalue": "group not found at path ''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/envs/nowcasting_dataset/lib/python3.9/site-packages/xarray/backends/zarr.py\u001b[0m in \u001b[0;36mopen_group\u001b[0;34m(cls, store, mode, synchronizer, group, consolidated, consolidate_on_close, chunk_store, storage_options, append_dim, write_region, safe_chunks, stacklevel)\u001b[0m\n\u001b[1;32m    363\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m                 \u001b[0mzarr_group\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzarr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_consolidated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mopen_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nowcasting_dataset/lib/python3.9/site-packages/zarr/convenience.py\u001b[0m in \u001b[0;36mopen_consolidated\u001b[0;34m(store, metadata_key, mode, **kwargs)\u001b[0m\n\u001b[1;32m   1182\u001b[0m     \u001b[0;31m# setup metadata store\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m     \u001b[0mmeta_store\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConsolidatedMetadataStore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadata_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nowcasting_dataset/lib/python3.9/site-packages/zarr/storage.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, store, metadata_key)\u001b[0m\n\u001b[1;32m   2589\u001b[0m         \u001b[0;31m# retrieve consolidated metadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2590\u001b[0;31m         \u001b[0mmeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson_loads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmetadata_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nowcasting_dataset/lib/python3.9/site-packages/zarr/storage.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    846\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 847\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '.zmetadata'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mGroupNotFoundError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_15068/856980436.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mds_from_zarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/home/jack/data/nwp.zarr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"zarr\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/nowcasting_dataset/lib/python3.9/site-packages/xarray/backends/api.py\u001b[0m in \u001b[0;36mopen_dataset\u001b[0;34m(filename_or_obj, engine, chunks, cache, decode_cf, mask_and_scale, decode_times, decode_timedelta, use_cftime, concat_characters, decode_coords, drop_variables, backend_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m     \u001b[0moverwrite_encoded_chunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"overwrite_encoded_chunks\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 495\u001b[0;31m     backend_ds = backend.open_dataset(\n\u001b[0m\u001b[1;32m    496\u001b[0m         \u001b[0mfilename_or_obj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[0mdrop_variables\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrop_variables\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nowcasting_dataset/lib/python3.9/site-packages/xarray/backends/zarr.py\u001b[0m in \u001b[0;36mopen_dataset\u001b[0;34m(self, filename_or_obj, mask_and_scale, decode_times, concat_characters, decode_coords, drop_variables, use_cftime, decode_timedelta, group, mode, synchronizer, consolidated, chunk_store, storage_options, stacklevel, lock)\u001b[0m\n\u001b[1;32m    822\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0mfilename_or_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_normalize_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename_or_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 824\u001b[0;31m         store = ZarrStore.open_group(\n\u001b[0m\u001b[1;32m    825\u001b[0m             \u001b[0mfilename_or_obj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m             \u001b[0mgroup\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nowcasting_dataset/lib/python3.9/site-packages/xarray/backends/zarr.py\u001b[0m in \u001b[0;36mopen_group\u001b[0;34m(cls, store, mode, synchronizer, group, consolidated, consolidate_on_close, chunk_store, storage_options, append_dim, write_region, safe_chunks, stacklevel)\u001b[0m\n\u001b[1;32m    379\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m                 )\n\u001b[0;32m--> 381\u001b[0;31m                 \u001b[0mzarr_group\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzarr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mopen_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mconsolidated\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0;31m# TODO: an option to pass the metadata_key keyword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nowcasting_dataset/lib/python3.9/site-packages/zarr/hierarchy.py\u001b[0m in \u001b[0;36mopen_group\u001b[0;34m(store, mode, cache_attrs, synchronizer, path, chunk_store, storage_options)\u001b[0m\n\u001b[1;32m   1166\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcontains_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1167\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mContainsArrayError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1168\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mGroupNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1170\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mGroupNotFoundError\u001b[0m: group not found at path ''"
     ]
    }
   ],
   "source": [
    "ds_from_zarr = xr.open_dataset(\"/home/jack/data/nwp.zarr\", engine=\"zarr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e01ee2-c24e-44ed-a085-67cc35fbc06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_from_zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d48a95b-7b89-4ce4-8151-e5aef7a840fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_from_zarr['t'].isel(step=6, time=1).plot.imshow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00995ad-5c1f-41fe-87ba-7ab678258add",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_from_zarr['t'].encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e172c71-0222-47ab-a46d-f58078649962",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_from_zarr['t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab8ba76-7aa3-4431-be48-1a561f56e3d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nowcasting_dataset",
   "language": "python",
   "name": "nowcasting_dataset"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
